[{"path":"https://simonpcouch.github.io/thonk/CLAUDE.html","id":"context---ellmer-chats","dir":"","previous_headings":"","what":"Context - ellmer chats","title":"NA","text":"?(ellmer::Chat) Chat package:ellmer R Documentation chat Description: Value: Methods: Public methods: Method ‘new()’: Method ‘get_turns()’: Method ‘set_turns()’: Method ‘add_turn()’: Method ‘get_system_prompt()’: Method ‘get_model()’: Method ‘set_system_prompt()’: Method ‘tokens()’: Method ‘last_turn()’: Method ‘chat()’: Method ‘chat_parallel()’: Method ‘extract_data()’: Method ‘extract_data_parallel()’: Method ‘extract_data_async()’: Method ‘chat_async()’: Method ‘stream()’: Method ‘stream_async()’: Method ‘register_tool()’: Method ‘clone()’: Examples:","code":"A ‘Chat’ is an sequence of sequence of user and assistant Turns  sent to a specific Provider. A ‘Chat’ is a mutable R6 object that  takes care of managing the state associated with the chat; i.e. it  records the messages that you send to the server, and the messages  that you receive back. If you register a tool (i.e. an R function  that the assistant can call on your behalf), it also takes care of  the tool loop.   You should generally not create this object yourself, but instead  call ‘chat_openai()’ or friends instead. A Chat object • ‘Chat$new()’       • ‘Chat$get_turns()’       • ‘Chat$set_turns()’       • ‘Chat$add_turn()’       • ‘Chat$get_system_prompt()’       • ‘Chat$get_model()’       • ‘Chat$set_system_prompt()’       • ‘Chat$tokens()’       • ‘Chat$last_turn()’       • ‘Chat$chat()’       • ‘Chat$chat_parallel()’       • ‘Chat$extract_data()’       • ‘Chat$extract_data_parallel()’       • ‘Chat$extract_data_async()’       • ‘Chat$chat_async()’       • ‘Chat$stream()’       • ‘Chat$stream_async()’       • ‘Chat$register_tool()’       • ‘Chat$clone()’ Usage:       Chat$new(provider, turns, seed = NULL, echo = \"none\")  Arguments:       ‘provider’ A provider object.       ‘turns’ An unnamed list of turns to start the chat with (i.e.,          continuing a previous conversation). If ‘NULL’ or          zero-length list, the conversation begins from scratch.       ‘seed’ Optional integer seed that ChatGPT uses to try and make          output more reproducible.       ‘echo’ One of the following options:             • ‘none’: don't emit any output (default when running in              a function).             • ‘text’: echo text output as it streams in (default              when running at the console).             • ‘all’: echo all input and output.           Note this only affects the ‘chat()’ method. Retrieve the turns that have been sent and received so far    (optionally starting with the system prompt, if any).  Usage:       Chat$get_turns(include_system_prompt = FALSE)   Arguments:       ‘include_system_prompt’ Whether to include the system prompt          in the turns (if any exists). Replace existing turns with a new list.  Usage:       Chat$set_turns(value)   Arguments:       ‘value’ A list of Turns. Add a pair of turns to the chat.  Usage:       Chat$add_turn(user, system)   Arguments:       ‘user’ The user Turn.       ‘system’ The system Turn. If set, the system prompt, it not, ‘NULL’.  Usage:       Chat$get_system_prompt() Retrieve the model name  Usage:       Chat$get_model() Update the system prompt  Usage:       Chat$set_system_prompt(value)   Arguments:       ‘value’ A string giving the new system prompt List the number of tokens consumed by each assistant turn.    Currently tokens are recorded for assistant turns only; so user    turns will have zeros.  Usage:       Chat$tokens() The last turn returned by the assistant.  Usage:       Chat$last_turn(role = c(\"assistant\", \"user\", \"system\"))   Arguments:       ‘role’ Optionally, specify a role to find the last turn with          for the role.   Returns:       Either a ‘Turn’ or ‘NULL’, if no turns with the specified role      have occurred. Submit input to the chatbot, and return the response as a simple    string (probably Markdown).  Usage:       Chat$chat(..., echo = NULL)   Arguments:       ‘...’ The input to send to the chatbot. Can be strings or          images (see ‘content_image_file()’ and          ‘content_image_url()’.       ‘echo’ Whether to emit the response to stdout as it is          received. If ‘NULL’, then the value of ‘echo’ set when the          chat object was created will be used. *[Experimental]*     Submit multiple prompts in parallel. Returns a list of Chat    objects, one for each prompt.  Usage:       Chat$chat_parallel(prompts, max_active = 10, rpm = 500)   Arguments:       ‘prompts’ A list of user prompts.       ‘max_active’ The maximum number of simultaenous requests to          send.       ‘rpm’ Maximum number of requests per minute. Extract structured data  Usage:       Chat$extract_data(..., type, echo = \"none\", convert = TRUE)   Arguments:       ‘...’ The input to send to the chatbot. Will typically include          the phrase \"extract structured data\".       ‘type’ A type specification for the extracted data. Should be          created with a ‘type_()’ function.       ‘echo’ Whether to emit the response to stdout as it is          received. Set to \"text\" to stream JSON data as it's          generated (not supported by all providers).       ‘convert’ Automatically convert from JSON lists to R data          types using the schema. For example, this will turn arrays          of objects into data frames and arrays of strings into a          character vector. *[Experimental]*     Submit multiple prompts in parallel. Returns a list of extracted    data, one for each prompt.  Usage:       Chat$extract_data_parallel(        prompts,        type,        convert = TRUE,        max_active = 10,        rpm = 500      )   Arguments:       ‘prompts’ A list of user prompts.       ‘type’ A type specification for the extracted data. Should be          created with a ‘type_()’ function.       ‘convert’ Automatically convert from JSON lists to R data          types using the schema. For example, this will turn arrays          of objects into data frames and arrays of strings into a          character vector.       ‘max_active’ The maximum number of simultaenous requests to          send.       ‘rpm’ Maximum number of requests per minute. Extract structured data, asynchronously. Returns a promise that    resolves to an object matching the type specification.  Usage:       Chat$extract_data_async(..., type, echo = \"none\")   Arguments:       ‘...’ The input to send to the chatbot. Will typically include          the phrase \"extract structured data\".       ‘type’ A type specification for the extracted data. Should be          created with a ‘type_()’ function.       ‘echo’ Whether to emit the response to stdout as it is          received. Set to \"text\" to stream JSON data as it's          generated (not supported by all providers). Submit input to the chatbot, and receive a promise that resolves    with the response all at once. Returns a promise that resolves    to a string (probably Markdown).  Usage:       Chat$chat_async(...)   Arguments:       ‘...’ The input to send to the chatbot. Can be strings or          images. Submit input to the chatbot, returning streaming results.    Returns A coro generator that yields strings. While iterating,    the generator will block while waiting for more content from the    chatbot.  Usage:       Chat$stream(...)   Arguments:       ‘...’ The input to send to the chatbot. Can be strings or          images. Submit input to the chatbot, returning asynchronously streaming    results. Returns a coro async generator that yields string    promises.  Usage:       Chat$stream_async(...)   Arguments:       ‘...’ The input to send to the chatbot. Can be strings or          images. Register a tool (an R function) that the chatbot can use. If the    chatbot decides to use the function, ellmer will automatically    call it and submit the results back.     The return value of the function. Generally, this should either    be a string, or a JSON-serializable value. If you must have more    direct control of the structure of the JSON that's returned, you    can return a JSON-serializable value wrapped in ‘base::I()’,    which ellmer will leave alone until the entire request is    JSON-serialized.  Usage:       Chat$register_tool(tool_def)   Arguments:       ‘tool_def’ Tool definition created by ‘tool()’. The objects of this class are cloneable with this method.  Usage:       Chat$clone(deep = FALSE)   Arguments:       ‘deep’ Whether to make a deep clone. chat <- chat_openai(echo = TRUE)  chat$chat(\"Tell me a funny joke\")"},{"path":"https://simonpcouch.github.io/thonk/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2025 Posit Software, PBC Permission hereby granted, free charge, person obtaining copy software associated documentation files (“Software”), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED “”, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":"https://simonpcouch.github.io/thonk/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Simon Couch. Author, maintainer. Posit Software, PBC. Copyright holder, funder.","code":""},{"path":"https://simonpcouch.github.io/thonk/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Couch S (2025). thonk: Explain Address Error Messages. R package version 0.0.0.9000, https://simonpcouch.github.io/thonk/, https://github.com/simonpcouch/thonk.","code":"@Manual{,   title = {thonk: Explain and Address Error Messages},   author = {Simon Couch},   year = {2025},   note = {R package version 0.0.0.9000, https://simonpcouch.github.io/thonk/},   url = {https://github.com/simonpcouch/thonk}, }"},{"path":"https://simonpcouch.github.io/thonk/index.html","id":"thonk-","dir":"","previous_headings":"","what":"Explain and Address Error Messages","title":"Explain and Address Error Messages","text":"goal thonk help users understand address error messages using LLMs. tool enabled, errors raised user accompanied clickable links “explain” “fix” issue. Explanations printed console fixes implement changes directly; cases, model supplied context files ’re working functions ’re working .","code":""},{"path":"https://simonpcouch.github.io/thonk/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Explain and Address Error Messages","text":"can install development version thonk like : enable thonk, call thonk::thonk_enable(). always thonk enabled every time start R, add thonk::thonk_enable() .Rprofile, perhaps usethis::edit_r_profile().","code":"pak::pak(\"simonpcouch/thonk\")"},{"path":"https://simonpcouch.github.io/thonk/index.html","id":"example","dir":"","previous_headings":"","what":"Example","title":"Explain and Address Error Messages","text":"following example, make mistake plotting mtcars:  Upon seeing error, click “explain” link , wrapping head around issue, allow model “fix” . model fixes code, runs correctly.","code":""},{"path":"https://simonpcouch.github.io/thonk/index.html","id":"thanks","dir":"","previous_headings":"","what":"Thanks","title":"Explain and Address Error Messages","text":"’d tossed package idea around various folks last months deciding give go: namely, Barret Schloerke Joshua Yamamoto.","code":""},{"path":"https://simonpcouch.github.io/thonk/reference/thonk-package.html","id":null,"dir":"Reference","previous_headings":"","what":"thonk: Explain and Address Error Messages — thonk-package","title":"thonk: Explain and Address Error Messages — thonk-package","text":"Helps users understand address error messages using large language models. tool enabled, errors raised user accompanied clickable links \"explain\" \"fix\" issue. Explanations printed console fixes implement changes directly; cases, model supplied context files working functions working .","code":""},{"path":[]},{"path":"https://simonpcouch.github.io/thonk/reference/thonk-package.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"thonk: Explain and Address Error Messages — thonk-package","text":"Maintainer: Simon Couch simon.couch@posit.co (ORCID) contributors: Posit Software, PBC [copyright holder, funder]","code":""},{"path":"https://simonpcouch.github.io/thonk/reference/thonk.html","id":null,"dir":"Reference","previous_headings":"","what":"Troubleshoot errors with LLMs — thonk","title":"Troubleshoot errors with LLMs — thonk","text":"thonk package provides tools automatically explaining fixing R errors using large language models (LLMs). error occurs, thonk can analyze error message, backtrace, context provide human-friendly explanation suggest fixes. thonk_enable(): Attaches global error handler captures errors provides clickable options explain fix . thonk_explain(): Explains recent error using LLM, offering detailed context went wrong . thonk_fix(): Attempts automatically fix recent error generating applying code fix. thonk can find relevant file lines, modify lines directly. Regardless, print proposed fix console.","code":""},{"path":"https://simonpcouch.github.io/thonk/reference/thonk.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Troubleshoot errors with LLMs — thonk","text":"","code":"thonk_enable(chat = getOption(\".thonk_chat\"))  thonk_explain()  thonk_fix()"},{"path":"https://simonpcouch.github.io/thonk/reference/thonk.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Troubleshoot errors with LLMs — thonk","text":"chat ellmer Chat object use interacting language model. provided, uses value getOption(\".thonk_chat\"). Set e.g. options(.thonk_chat = ellmer::chat_claude(model = \"claude-3-7-sonnet-latest\")) .Rprofile.","code":""},{"path":"https://simonpcouch.github.io/thonk/reference/thonk.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Troubleshoot errors with LLMs — thonk","text":"","code":"if (FALSE) { # \\dontrun{ # Attach the error handler at the start of your session: thonk_enable()  # Code that will error: sum(1, \"n\")  # If an error occurs, you'll get interactive links to explain or fix # Alternatively, you can call these functions directly: thonk_explain() thonk_fix() } # }"}]
